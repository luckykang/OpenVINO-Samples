模型优化器调优
DLDT--Deep Learning Deployment Toolkit  深度学习部署工具包
它包括模型优化器(Model Optimizer)和推理引擎(Inference Engine)

两级优化：
第一级是Model Optimizer对tf、caffe、mxnet、onnx等模型的优化，生成IR文件。模型优化器的使用和硬件没有关系。
第二级是Inference Engine，针对特定的硬件设备进行优化。

Model Optimizer是一个离线工具，运行一次就可以生成IR文件。查找所有层的实施方案，比如axpy的caffe层(在文档GUIDES中的Supported Framework Layers的
。打开/opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/caffe/axpy.py文件，查看实现这一层的代码。

Model Optimizer有哪些功能：
1.可以对深度学习框架下的模型进行转换
2.将模型网络映射到支持的库、类和或者层
3.添加动作



mo.py
    --scale_values [R,G,B]
深度网络往往不会直接使用图像像素，而是先对其进行校转化和缩放，通常会将每个训练的图像减去被用于训练图像的平均值。scale_values的值可以帮助把每个通道的数值除以不同的数值，
通道代表的信息和顺序取决于原始模型受训练的方式，每个通道减去平均值的操作也很容易。
    --reverse_input_channel 
对输入的顺序进行反转，比如将RGB转换为BGR，必须要注意的是模型的输入通道的顺序需要和推理图片的通道顺序保持一致
4.优化神经网络
mo.py
    --disable_fusing
如果你对模型优化器不敢兴趣，只需要运行模型优化器的禁用融合功能，就可以不使用模型优化器对模型所做的优化。
5.更改格式
模型优化器可以将数据和权重的格式从FP32更改为FP16，从浮点到整数的转换是通过模型优化器的外部专用校准流程来完成的。
6.剪切部分网络
mo.py
    --input pool1
    模型优化器可以剪切网络的相应部分，比如运行--input pool1剪切pool1之前的所有层







7.支持自定义层

实验一：了解模型优化器的输入参数
下载 mobilenetv2-7.onnx  分类
wget https://github.com/onnx/models/raw/master/vision/classification/mobilenet/model/mobilenetv2-7.onnx
录入模型优化器路径
export mo_dir=/opt/intel/openvino/deployment_tools/model_optimizer/
模型转换   添加均值 --mean_values 和缩放值 --scale_values，并将RGB反转为BGR --reverse_input_channels
mo.py --input_model mobilenetv2-7.onnx --mean_values=data[123.675,116.28,103.53] --scale_values=data[58.624,57.12,57.375]  --reverse_input_channels -o $lab_dir
运行推理
python3 classification_sample.py -m mobilenetv2-7.xml --labels labels.txt -i images/1.jpeg


实验二：
从 download.py 下载:ssd_mobilenet_v2_coco  
录入模型优化器路径
export mo_dir=/opt/intel/openvino/deployment_tools/model_optimizer/
运行模型优化器
mo.py --input_model public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb --reverse_input_channels --input_shape [1,300,300,3] --input=image_tensor --transformations_config=$mo_dir/extensions/front/tf/ssd_v2_support.json --tensorflow_object_detection_api_pipeline_config=public/ssd_mobilenet_v2_coco/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config --output=detection_classes,detection_scores,detection_boxes,num_detections --model_name ssd-mobilenet
运行目标检测sample
python3 object_detection.py -m ssd-mobilenet.xml --labels labels.txt -i images/1.jpg


实验三：深入了解模型优化器 classification model（ONNX）
录入模型优化器路径
export mo_dir=/opt/intel/openvino/deployment_tools/model_optimizer/
使用模型优化器生成batch为4的IR
使用--batch 参数，并设置值为 4 ：
mo.py  --input_model mobilenetv2-7.onnx --mean_values=data[123.675,116.28,103.53] --scale_values=data[58.624,57.12,57.375] --reverse_input_channels --output_dir $lab_dir --batch 4 --model_name mobilenetv2-7-batch-4
使用模型优化器生成特定input模型：
使用input_shape参数设置输出size为[1,3,100,100]:
mo.py  --input_model mobilenetv2-7.onnx --mean_values=data[123.675,116.28,103.53] --scale_values=data[58.624,57.12,57.375] --reverse_input_channels --output_dir $lab_dir --input_shape [1,3,100,100]  --model_name mobilenetv2-7-smaller
使用原始模型进行分类推理：
python3 classification_sample.py -m mobilenetv2-7.xml --labels labels.txt -i 1.JPEG
使用smaller模型进行推理：
python3 classification_sample.py -m mobilenetv2-7-smaller.xml --labels labels.txt -i 1.JPEG
剪切模型，去掉模型前5层：
将模型第六层"mobilenetv20_features_conv0_fwd"设置为模型优化器的--input 的参数，意味着我们剪切掉模型的前5层：
mo.py --input_model mobilenetv2-7.onnx --mean_values=data[123.675,116.28,103.53] --scale_values=data[58.624,57.12,57.375] --reverse_input_channels --output_dir $lab_dir --input mobilenetv20_features_conv0_fwd  --model_name mobilenetv2-7-no-head
用模型优化器将模型转化为FP16精度：
mo.py --input_model mobilenetv2-7.onnx --mean_values=data[123.675,116.28,103.53] --scale_values=data[58.624,57.12,57.375] --reverse_input_channels --output_dir $lab_dir --data_type FP16 --model_name mobilenetv2-7-FP16
使用FP16模型运行sample：
python3 classification_sample.py -m mobilenetv2-7-FP16.xml   --labels labels.txt -i 1.JPEG

