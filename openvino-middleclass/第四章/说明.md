一.推理引擎介绍：
针对特定执行设备对模型进行优化，这些设备具有完全不同的指令集，有的还有不同的内存、布局等。
每个设备或者设备系列都有自己的实现方案。例如cpu插件负责在凌动、酷睿、至强系列的芯片执行Inference Engine。
每个插件都有自己的实现库，例如cpu插件使用MKL-DNN库，MKL-DNN库会针对所有的intel cpu对应的内核、层或函数实施神经网络的优化，如果该库不支持你的层，你可以构建一个自定义层，并将其注册到推理引擎。

二.推理引擎是怎么优化的：
1.网络级优化，不映射到内核，而是映射到他们之间的关系。例如数据的重组，这可以提高性能，并在推理过程中最大程度的减少数据转换时间
2.内存级优化，内存中按照特定的设备的要求重组数据
3.内核级优化，推理引擎选择最适合架构指令集的正确实施方案。
此外还有两个特殊插件，HETERO和MULTI。可以通过这两个特殊插件访问所有的设备。

![0827134402](https://cdn.jsdelivr.net/gh/luckykang/picture_bed/blogs_images/0827134402.png)

三.Inference Engine API
Inference Engine API是在所有Intel架构的硬件中实施推理的一套简单而且统一的API，正如我们所解释的，特殊的插件架构支持优化推理性能和内存使用，该API非常简单，同时具有足够的灵活性来支持所需的一切。主要使用c++实施，相对使用同样提供的python接口会快很多。

IECore：
是Inference Engine的主要类，运行在各种不同插件的上层，并向开发者提供相同的功能。可以创建一个Core类的对象，并只有加载网络时指定设备，实际运行才会使用该设备，无需专门注册特定插件。
InferRequest:
然后使用InferRequest进行推理
HETERO plugin：
HETERO plugin支持在不同设备上运行不受支持的特定层。有时候目标设备不支持所有的层，HETERO plugin可以将不支持层的执行回退到其他设备。
例如：
exex_net = my_ie.load_network(network=net,device_name='HETERO:FPGA,CPU')   表示尝试在FPGA上运行所有层，如果找不到任何层的实施方案，就在CPU设备运行相应层。
MULTI plugin:
当程序运行时生成许多推理请求，例如在视频上运行推理，每帧都具有不同的推理调用，甚至一帧中有几个推理的调用。因此，MULTI plugin可以让你在不同的设备上运行几个推理调用，从而利用系统中的所有设备。
例如：
exex_net = my_ie.load_network(network=net,device_name='MULTI:MYRIAD,CPU')  所有推理请求发送到MYRIAD，但是当MYRIAD充分利用时，后续的推理请求将发送到CPU，这样MYRIAD和CPU并行执行推理，并且可以提供更好的性能，该插件支持我们根据设备的实时可有性来使用他们。

四.Inference Engine还可以做什么：
可以报告实际的运行时性能计数器，还可以感知哪些设备已连接并可以使用，还可以获取实际物理状况的实时指示。







